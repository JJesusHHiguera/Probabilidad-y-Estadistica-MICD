{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tarea 5: Práctica de Spark en la nube**\r\n",
        "\r\n",
        "## Alumno: José de Jesús Hernández Higuera\r\n",
        "\r\n",
        "### Matrícula: 224470489"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comencemos por colocar la configuración de Azure para poder obtener los datos requeridos más adelante (este código fue obtenido directamente de la página de Microsoft Azure)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Azure storage access info\r\n",
        "blob_account_name = \"azureopendatastorage\"\r\n",
        "blob_container_name = \"nyctlc\"\r\n",
        "blob_relative_path = \"yellow\"\r\n",
        "blob_sas_token = r\"\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 26,
              "statement_ids": [
                26
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:18:42.7077555Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:18:42.8380984Z",
              "execution_finish_time": "2024-12-06T06:18:42.9828234Z",
              "parent_msg_id": "e06bb22e-df95-41a6-a5f5-7d37f54daaa1"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 26, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuración de Spark para leer los datos\r\n",
        "spark.conf.set(\r\n",
        "    \"fs.azure.sas.datos-pye.clase211124.blob.core.windows.net\",\r\n",
        "    \"sp=r&st=2024-12-06T05:05:25Z&se=2024-12-06T13:05:25Z&sv=2022-11-02&sr=b&sig=JkJrW7sIzXJH67XMYnaElr1PthX0NGUE8PTvKP9YoPI%3D\"\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 27,
              "statement_ids": [
                27
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:18:42.7087189Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:18:43.0951169Z",
              "execution_finish_time": "2024-12-06T06:18:43.2598555Z",
              "parent_msg_id": "7dc56c30-201e-47e0-befb-e96fea733c9f"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 27, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3a. Update the `user()` function\r\n",
        "This function should return your username, eg: janedoe3"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user():\r\n",
        "    # Returns a string consisting of your username.\r\n",
        "    return 'José de Jesús Hernández Higuera'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 28,
              "statement_ids": [
                28
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:18:42.7102463Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:18:43.3848479Z",
              "execution_finish_time": "2024-12-06T06:18:43.5313682Z",
              "parent_msg_id": "9261e2bc-4562-448f-a60a-cbc71aaff5b6"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 28, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3b. Update the `long_trips()` function\r\n",
        "This function filters trips to keep only trips longer than 2 miles."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "def long_trips(trips):\r\n",
        "    #Returns a Dataframe with Schema the same as :trips:\r\n",
        "    #Filtremos aquellos viajes mayores a 2 millas\r\n",
        "    filt_trips = trips.filter(F.col('tripDistance') >= 2)\r\n",
        "    return filt_trips"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 29,
              "statement_ids": [
                29
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:18:42.7114939Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:18:43.6461589Z",
              "execution_finish_time": "2024-12-06T06:18:43.801238Z",
              "parent_msg_id": "cc259eb4-e55a-4a10-987d-9c950126b681"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 29, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3c. Update the `manhattan_trips()` function\r\n",
        "\r\n",
        "This function determines the top 20 locations with a `DOLocationID` in manhattan by passenger_count (pcount).\r\n",
        "\r\n",
        "Example output formatting:\r\n",
        "\r\n",
        "```\r\n",
        "+--------------+--------+\r\n",
        "| DOLocationID | pcount |\r\n",
        "+--------------+--------+\r\n",
        "|             5|      15|\r\n",
        "|            16|      12| \r\n",
        "+--------------+--------+\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "def manhattan_trips(trips, lookup):\r\n",
        "    #Returns a Dataframe with Schema: DOLocationID, pcount\r\n",
        "    #Filtremos para obtener los ID's de Manhattan\r\n",
        "    manh = lookup.filter(F.col('Borough') == 'Manhattan').select('LocationID')\r\n",
        "    #Unamos los dataframes mediante los ID's que coincidan \r\n",
        "    join_df = trips.join(manh, trips['doLocationId'] == manh['LocationID'], how='inner')\r\n",
        "    #Seleccionemos las columnas requeridas, ordenemos respecto al conteo de pasajeros y obtengamos las 20 ubicaciones más populares\r\n",
        "    manh_trips = join_df.withColumn('pcount', F.col('passengerCount')).select('doLocationID', 'pcount').orderBy(F.col('pcount').desc()).limit(20)\r\n",
        "    #Guardemos este resultado en caché\r\n",
        "    manh_trips = manh_trips.cache()\r\n",
        "    return manh_trips"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 35,
              "statement_ids": [
                35
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:19:20.0593377Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:19:20.2028349Z",
              "execution_finish_time": "2024-12-06T06:19:20.3489798Z",
              "parent_msg_id": "84a5637f-0644-4ff3-bad2-4940b688ffe0"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 35, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3d. Update the `weighted_profit()` function\r\n",
        "This function should determine the average `total_amount`, the total count of trips, and the total count of trips ending in the top 20 destinations and return the `weighted_profit` as discussed in the homework document.\r\n",
        "\r\n",
        "Example output formatting:\r\n",
        "```\r\n",
        "+--------------+-------------------+\r\n",
        "| PULocationID |  weighted_profit  |\r\n",
        "+--------------+-------------------+\r\n",
        "|            18| 33.784444421924436| \r\n",
        "|            12| 21.124577637149223| \r\n",
        "+--------------+-------------------+\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "def weighted_profit(trips, mtrips): \r\n",
        "    #Returns a Dataframe with Schema: PULocationID, weighted_profit\r\n",
        "    #Calculemos el promedio de 'total_amount' (average total amount)\r\n",
        "    av_t_a = trips.groupBy('puLocationId').agg(F.avg('totalAmount').alias('average_total_amount'))\r\n",
        "    #Contemos el total de viajes (total count of trips)\r\n",
        "    tct = trips.groupBy('puLocationId').agg(F.count('*').alias('total_count_trips'))\r\n",
        "    #Contemos el número total de viajes que terminan en el top 20 de destinos más populares (total count of trips ending in the top 20 destinations)\r\n",
        "    tct20 = trips.join(mtrips, trips['doLocationId'] == mtrips['doLocationId'],\r\n",
        "                       how='inner').groupBy('PULocationID').agg(F.count('*').alias('total_count_trips_top_20destinations'))\r\n",
        "    #Unamos los resultados anteriores para formar el dataframe deseado\r\n",
        "    w_profit = av_t_a.join(tct, \"puLocationId\", how=\"inner\").join(tct20, \"puLocationId\", how=\"left\")\r\n",
        "    #Obtengamos el 'weighted profit'\r\n",
        "    w_profit = w_profit.withColumn('weighted_profit',\r\n",
        "                                   (F.col('total_count_trips_top_20destinations') / F.col(\"total_count_trips\")) * F.col('average_total_amount'))\r\n",
        "    #Seleccionemos las columnas necesarias y guardemos este resultado en Cache\r\n",
        "    w_profit = w_profit.select('PULocationID', 'weighted_profit').cache()\r\n",
        "    return w_profit"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 37,
              "statement_ids": [
                37
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:21:01.1496603Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:21:01.9144295Z",
              "execution_finish_time": "2024-12-06T06:21:02.0587841Z",
              "parent_msg_id": "cd0b9eea-6251-4d95-a35f-0230fbc3e066"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 37, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3e. Update the `final_output()` function\r\n",
        "This function will take the results of `weighted_profit`, links it to the `borough` and `zone` and returns the top 20 locations with the highest `weighted_profit`.\r\n",
        "\r\n",
        "Example output formatting:\r\n",
        "```\r\n",
        "+------------+---------+-------------------+\r\n",
        "|    Zone    | Borough |  weighted_profit  |\r\n",
        "+----------------------+-------------------+\r\n",
        "| JFK Airport|   Queens|  16.95897820117925|\r\n",
        "|     Jamaica|   Queens| 14.879835188762488|\r\n",
        "+------------+---------+-------------------+\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "def final_output(calc, lookup): \r\n",
        "    #Returns a Dataframe with Schema: Zone, Borough, weighted_profit\r\n",
        "    #Unimos los dataframes necesarios por medio del ID, seleccionamos las columnas solicitadas y obtenemos las 20 con mayor 'weighted_profit'\r\n",
        "    final_output = lookup.join(calc, calc['puLocationId'] == lookup['LocationID'], how='inner').select('Zone', 'Borough', 'weighted_profit')\\\r\n",
        "        .orderBy(F.col('weighted_profit').desc()).limit(20)\r\n",
        "    return final_output"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 32,
              "statement_ids": [
                32
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:18:42.7155883Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:18:44.4685032Z",
              "execution_finish_time": "2024-12-06T06:18:44.6108322Z",
              "parent_msg_id": "dca8dac6-caf3-49e7-b278-d3dfa91be33f"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 32, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente celda implementa las funciones definidas anteriormente y, además, carga los datos necesarios para esta práctica."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### DO NOT CHANGE ANYTHING IN THIS CELL ####\r\n",
        "\r\n",
        "def load_data():\r\n",
        "    # Loads the data for this question. Do not change this function.\r\n",
        "\r\n",
        "    # Load Trip Data\r\n",
        "    #Allow SPARK to read from Blob remotely\r\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "    spark.conf.set(\r\n",
        "    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n",
        "    blob_sas_token)\r\n",
        "    print('Remote blob path: ' + wasbs_path)\r\n",
        "\r\n",
        "    #SPARK read parquet, note that it won't load any data yet by now\r\n",
        "    trips = spark.read.parquet(wasbs_path)\r\n",
        "\r\n",
        "    # Load Lookup Data\r\n",
        "    file_path = \"wasbs://datos-pye@clase211124.blob.core.windows.net/taxi_zone_lookup.csv\"\r\n",
        "    lookup = spark.read.csv(file_path, header=True, inferSchema=True)\r\n",
        "\r\n",
        "    return trips, lookup\r\n",
        "\r\n",
        "def main():\r\n",
        "    # Runs your functions implemented above.\r\n",
        "    \r\n",
        "    print(user())\r\n",
        "    trips, lookup = load_data()\r\n",
        "    trips = long_trips(trips)\r\n",
        "    mtrips = manhattan_trips(trips, lookup)\r\n",
        "    wp = weighted_profit(trips, mtrips)\r\n",
        "    final = final_output(wp,lookup)\r\n",
        "    \r\n",
        "    # Outputs the results for you to visually see\r\n",
        "    final.show()\r\n",
        "    \r\n",
        "    # Writes out as a CSV to your bucket.\r\n",
        "    #final.write.csv(bucket)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 33,
              "statement_ids": [
                33
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:18:42.7168307Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:18:44.7222411Z",
              "execution_finish_time": "2024-12-06T06:18:44.8732227Z",
              "parent_msg_id": "116a1e61-79a2-4a4c-a3df-4e115d8e95fe"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 33, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, pasemos a la ejecución del código en la base de datos completa."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clusterprueba",
              "statement_id": 38,
              "statement_ids": [
                38
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T06:21:07.6658571Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T06:21:07.7742198Z",
              "execution_finish_time": "2024-12-06T06:26:44.1062796Z",
              "parent_msg_id": "925d4893-d75b-4197-9fb3-8fc4872ce2ed"
            },
            "text/plain": "StatementMeta(clusterprueba, 1, 38, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "José de Jesús Hernández Higuera\nRemote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\n+--------------------+-------------+------------------+\n|                Zone|      Borough|   weighted_profit|\n+--------------------+-------------+------------------+\n|        Baisley Park|       Queens|21.304881627933888|\n|       South Jamaica|       Queens| 20.37931200203064|\n|Flushing Meadows-...|       Queens| 16.21832222972873|\n|             Jamaica|       Queens|15.014703301065234|\n|Springfield Garde...|       Queens|14.916668177101627|\n|Briarwood/Jamaica...|       Queens|13.645051709988033|\n|   LaGuardia Airport|       Queens|12.485102264520407|\n|         JFK Airport|       Queens|12.467055000043741|\n|              Corona|       Queens|11.961650377504567|\n|     Randalls Island|    Manhattan|11.171858909273054|\n|Financial Distric...|    Manhattan|10.530174480827634|\n|Financial Distric...|    Manhattan|10.301776606950959|\n|             Maspeth|       Queens|10.160308389604852|\n|         Jamaica Bay|       Queens|10.111962369801864|\n|        Battery Park|    Manhattan| 9.872629047659032|\n|   Battery Park City|    Manhattan| 9.764026991969736|\n|  World Trade Center|    Manhattan| 9.663052207594907|\n|      Yorkville East|    Manhattan| 9.583055746005753|\n|             Seaport|    Manhattan| 9.211150851687293|\n|    Great Kills Park|Staten Island| 8.941632653061225|\n+--------------------+-------------+------------------+\n\n"
          ]
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": "Práctica de Spark en la nube",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}